{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSwrMWxSFzTH"
      },
      "outputs": [],
      "source": [
        "!pip -q install speechrecognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gtts"
      ],
      "metadata": {
        "id": "FGMngmo2F39J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import json\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gtts import gTTS\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipd\n",
        "import speech_recognition as sr\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Flatten, Dense, GlobalMaxPool1D"
      ],
      "metadata": {
        "id": "4yawTkxfF6IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Package sentence tokenizer\n",
        "nltk.download('punkt')\n",
        "# Package lemmatization\n",
        "nltk.download('wordnet')\n",
        "# Package multilingual wordnet data\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "Q5lV5HCPF9X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lQHNcgdiGBUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('/content/drive/MyDrive/DATASET_PI/kampus_merdeka.json')\n",
        "df.shape"
      ],
      "metadata": {
        "id": "NjQFa75HGDde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "with open('/content/drive/MyDrive/DATASET_PI/kampus_merdeka.json') as content:\n",
        "  data1 = json.load(content)\n",
        "\n",
        "# Mendapatkan semua data ke dalam list\n",
        "tags = [] # data tag\n",
        "inputs = [] # data input atau pattern\n",
        "responses = {} # data respon\n",
        "words = [] # Data kata\n",
        "classes = [] # Data Kelas atau Tag\n",
        "documents = [] # Data Kalimat Dokumen\n",
        "ignore_words = ['?', '!'] # Mengabaikan tanda spesial karakter\n",
        "# Tambahkan data intents dalam json\n",
        "for intent in data1['intents']:\n",
        "  responses[intent['tag']]=intent['responses']\n",
        "  for lines in intent['patterns']:\n",
        "    inputs.append(lines)\n",
        "    tags.append(intent['tag'])\n",
        "    # digunakan untuk pattern atau teks pertanyaan dalam json\n",
        "    for pattern in intent['patterns']:\n",
        "      w = nltk.word_tokenize(pattern)\n",
        "      words.extend(w)\n",
        "      documents.append((w, intent['tag']))\n",
        "      # tambahkan ke dalam list kelas dalam data\n",
        "      if intent['tag'] not in classes:\n",
        "        classes.append(intent['tag'])\n",
        "\n",
        "# Konversi data json ke dalam dataframe\n",
        "data = pd.DataFrame({\"patterns\":inputs, \"tags\":tags})"
      ],
      "metadata": {
        "id": "bw2dO60kGFlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cetak data keseluruhan\n",
        "data"
      ],
      "metadata": {
        "id": "LTqybr8rGJU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cetak data baris pertama sampai baris kelima\n",
        "data.head()"
      ],
      "metadata": {
        "id": "DJjWvtE-GM9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cetak data baris ke-70 sampai baris akhir\n",
        "data.tail()"
      ],
      "metadata": {
        "id": "AW2Z7ScwGQUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Punctuations (Menghilangkan Punktuasi)\n",
        "data['patterns'] = data['patterns'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
        "data['patterns'] = data['patterns'].apply(lambda wrd: ''.join(wrd))"
      ],
      "metadata": {
        "id": "0xgbLfpMGSWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "print (len(words), \"unique lemmatized words\", words)"
      ],
      "metadata": {
        "id": "mvAIMDXkGVX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sorting pada data class\n",
        "classes = sorted(list(set(classes)))\n",
        "print (len(classes), \"classes\", classes)"
      ],
      "metadata": {
        "id": "UaO7zv3GGYJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = kombinasi antara data pattern dengan data tag dalam intents json\n",
        "print (len(documents), \"documents\")"
      ],
      "metadata": {
        "id": "5cagA-FHGaYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data (Tokenisasi Data)\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(data['patterns'])\n",
        "train = tokenizer.texts_to_sequences(data['patterns'])\n",
        "train"
      ],
      "metadata": {
        "id": "YadInh5CGchb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan proses padding pada data\n",
        "x_train = pad_sequences(train)\n",
        "# Menampilkan hasil padding\n",
        "print(x_train)"
      ],
      "metadata": {
        "id": "LN5lp3dbGfmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan konversi data label tags dengan encoding\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(data['tags'])\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "xVA15XheGh_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat hasil input pada data teks\n",
        "input_shape = x_train.shape[1]\n",
        "print(input_shape)"
      ],
      "metadata": {
        "id": "NqOkV0G0GkA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan definisi tiap kalimat dan kata pada data teks\n",
        "vocabulary = len(tokenizer.word_index)\n",
        "print(\"number of unique words : \", vocabulary)\n",
        "\n",
        "# Melakukan pemeriksaan pada data output label teks\n",
        "output_length = le.classes_.shape[0]\n",
        "print(\"output length: \", output_length)"
      ],
      "metadata": {
        "id": "MaVut7OnGmFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan hasil pemrosesan teks dengan menggunakan pickle\n",
        "pickle.dump(words, open('words.pkl','wb'))\n",
        "pickle.dump(classes, open('classes.pkl','wb'))"
      ],
      "metadata": {
        "id": "RYd6gx5xGoE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(le, open('le.pkl','wb'))\n",
        "pickle.dump(tokenizer, open('tokenizers.pkl','wb'))"
      ],
      "metadata": {
        "id": "APrHvWqaGwtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model (Membuat Modelling)\n",
        "i = Input(shape=(input_shape,)) # Layer Input\n",
        "x = Embedding(vocabulary+1,10)(i) # Layer Embedding\n",
        "x = LSTM(10, return_sequences=True, recurrent_dropout=0.2)(x) # Layer Long Short Term Memory\n",
        "x = Flatten()(x) # Layer Flatten\n",
        "x = Dense(output_length, activation=\"softmax\")(x) # Layer Dense\n",
        "model  = Model(i,x) # Model yang telah disusun dari layer Input sampai layer Output\n",
        "\n",
        "# Compiling the model (Kompilasi Model)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "js9eGhr9GxTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization Plot Architecture Model (Visualisasi Plot Arsitektur Model)\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "volZp3H2G056"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan parameter pada model LSTM\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nZQrf3keG3L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model (Melatih model data sampai 450 kali)\n",
        "train = model.fit(x_train, y_train, epochs=450)"
      ],
      "metadata": {
        "id": "Sopemp5lG5Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting model Accuracy and Loss (Visualisasi Plot Hasil Akurasi dan Loss)\n",
        "# Plot Akurasi\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train.history['accuracy'], label='Training Set Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Accuracy')\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train.history['loss'], label='Training Set Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LF2yT_fxG7tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat Input Chat\n",
        "while True:\n",
        "  texts_p = []\n",
        "  prediction_input = input('👩🏻‍🦰 Ada yang bisa dibantu : ')\n",
        "\n",
        "  # Menghapus punktuasi atau tanda baca dan konversi ke huruf kecil\n",
        "  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
        "  prediction_input = ''.join(prediction_input)\n",
        "  texts_p.append(prediction_input)\n",
        "\n",
        "  # Melakukan Tokenisasi dan Padding pada data teks\n",
        "  prediction_input = tokenizer.texts_to_sequences(texts_p)\n",
        "  # Konversi data teks menjadi array\n",
        "  prediction_input = np.array(prediction_input).reshape(-1)\n",
        "  prediction_input = pad_sequences([prediction_input],input_shape)\n",
        "\n",
        "  # Mendapatkan hasil prediksi keluaran pada model\n",
        "  output = model.predict(prediction_input)\n",
        "  output = output.argmax()\n",
        "\n",
        "  # Menemukan respon sesuai data tag dan memainkan suara bot\n",
        "  response_tag = le.inverse_transform([output])[0]\n",
        "  # Bot akan melakukan random jawaban percakapan dari hasil pertanyaan\n",
        "  print(\"🤖 Norabot : \", random.choice(responses[response_tag]))\n",
        "  # Tambahkan suara bot dengan Google Text to Speech\n",
        "  tts = gTTS(random.choice(responses[response_tag]), lang='id')\n",
        "  # Simpan model voice bot ke dalam Google Drive dengan format .wav\n",
        "  tts.save('Norabot.wav')\n",
        "  # Atur waktu jeda sampai 8 detik\n",
        "  time.sleep(0.08)\n",
        "  # Ambil file model yang telah disimpan sebelumnya\n",
        "  ipd.display(ipd.Audio('/content/Norabot.wav', autoplay=False))\n",
        "  print(\"=\"*60 + \"\\n\")\n",
        "  # Tambahkan respon 'goodbye' agar bot bisa berhenti melakukan percakapan\n",
        "  if response_tag == \"goodbye\":\n",
        "    break"
      ],
      "metadata": {
        "id": "PEl8_i93G9lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model dalam bentuk format file .h5 atau .pkl (pickle)\n",
        "model.save('chat_model.h5')\n",
        "\n",
        "print('Model Created Successfully!')"
      ],
      "metadata": {
        "id": "GffOgrecHBlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}